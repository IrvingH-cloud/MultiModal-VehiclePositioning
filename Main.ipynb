{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "deaa3b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To read in the used libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.io \n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import scipy.stats as stats\n",
    "import higher\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' \n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0681bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#                Dataloader                 #\n",
    "#############################################\n",
    "\n",
    "#Read in the unlabeled multi-modal dataset\n",
    "data_tensor = np.load('multi_modal_trainSet.npy',allow_pickle=True)\n",
    "\n",
    "\n",
    "#Read in the labeled datasets\n",
    "train_dl_BS1 = torch.load('Datasets/train_dl_BS1_300.pt')\n",
    "valid_dl_BS1 = torch.load('Datasets/valid_dl_BS1_100.pt')\n",
    "testing_dl_BS1 = torch.load('Datasets/testing_dl_BS1.pt')\n",
    "\n",
    "train_dl_BS2 = torch.load('Datasets/train_dl_BS2_300.pt')\n",
    "valid_dl_BS2 = torch.load('Datasets/valid_dl_BS2_100.pt')\n",
    "testing_dl_BS2 = torch.load('Datasets/testing_dl_BS2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "48f25043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#                Tool Functions             #\n",
    "#############################################\n",
    "\n",
    "#Divide a bag of data into sections\n",
    "def dividing(data_array,num_array):\n",
    "    if data_array.shape[0]!=sum(num_array):\n",
    "        return 'wrong!'\n",
    "    result = []\n",
    "    summing = 0\n",
    "    for item in num_array:\n",
    "        result.append(data_array[summing:summing+item])\n",
    "        summing = summing + item\n",
    "    return result\n",
    "\n",
    "#Metric\n",
    "def metric_func(y_pred,y_true):\n",
    "\n",
    "    validSize = y_true.shape[0]\n",
    "    position_gt_x = y_true[:,0]\n",
    "    position_gt_y = y_true[:,1]\n",
    "\n",
    "    position_pred_x = y_pred[:,0]\n",
    "    position_pred_y = y_pred[:,1]\n",
    "\n",
    "    deltaX_square = torch.mul(position_gt_x-position_pred_x,position_gt_x-position_pred_x)\n",
    "    deltaY_square = torch.mul(position_gt_y-position_pred_y,position_gt_y-position_pred_y)\n",
    "    deltaDistance = torch.sqrt(deltaX_square + deltaY_square)\n",
    "    mean_distanceError = torch.sum(deltaDistance)/validSize\n",
    "    return mean_distanceError\n",
    "\n",
    "\n",
    "#E step with KM algorithm\n",
    "def E_step(position_pred,position_image,stdDeviation_matrix):\n",
    "    num_pred = position_pred.shape[0]\n",
    "    num_image = position_image.shape[0]\n",
    "    position_pred_duplicate = position_pred.repeat(1,num_image).reshape(num_pred,num_image,2)\n",
    "    position_image_duplicate = position_image.repeat(1,num_pred).reshape(num_image,num_pred,2).permute(1,0,2)\n",
    "    root_cost_matrix = (torch.norm(position_pred_duplicate-position_image_duplicate,dim=2)/stdDeviation_matrix).numpy()\n",
    "    row_ind, col_ind = linear_sum_assignment(root_cost_matrix*root_cost_matrix)\n",
    "    return row_ind, col_ind, root_cost_matrix*root_cost_matrix\n",
    "\n",
    "\n",
    "#E step with Nearest neighbour\n",
    "def Nearest(position_pred,position_image,stdDeviation_matrix):\n",
    "    num_pred = position_pred.shape[0]\n",
    "    num_image = position_image.shape[0]\n",
    "    position_pred_duplicate = position_pred.repeat(1,num_image).reshape(num_pred,num_image,2)\n",
    "    position_image_duplicate = position_image.repeat(1,num_pred).reshape(num_image,num_pred,2).permute(1,0,2)\n",
    "    root_cost_matrix = (torch.norm(position_pred_duplicate-position_image_duplicate,dim=2)/stdDeviation_matrix)\n",
    "    row_ind = torch.tensor(range(num_pred))\n",
    "    col_ind = torch.min(root_cost_matrix*root_cost_matrix,dim=1).indices\n",
    "    return row_ind, col_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d51957bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#                Neural Networks            #\n",
    "#############################################\n",
    "\n",
    "class Residual(nn.Module):  \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Residual, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=(1,2))\n",
    "        self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=(1,2))\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        residual = self.conv3(residual)\n",
    "        out = x + self.bn3(residual)\n",
    "        return out\n",
    " \n",
    "\n",
    "def resnet_block(in_channels, out_channels):\n",
    "    blk = []\n",
    "    blk.append(Residual(in_channels, out_channels))\n",
    "    return nn.Sequential(*blk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "51dd8903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#                Generate the NN            #\n",
    "#############################################\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#Whether using a pretrained model\n",
    "pretraining = True\n",
    "\n",
    "if pretraining is False:\n",
    "    \n",
    "    model1_BS1 = nn.Sequential()\n",
    "    model1_BS1.add_module(\"resnet_block1\", resnet_block(3, 32))\n",
    "    model1_BS1.add_module(\"resnet_block2\", resnet_block(32, 64))\n",
    "    model1_BS1.add_module(\"flatten\",nn.Flatten())\n",
    "    model1_BS1.add_module(\"Dropout\",nn.Dropout(0.5))\n",
    "    model1_BS1.add_module(\"linear0\",nn.Linear(64*16*13,32))\n",
    "    model1_BS1.add_module(\"linear1\",nn.Linear(32,128))\n",
    "    model1_BS1.add_module(\"ReLU1\",nn.ReLU())\n",
    "    model1_BS1.add_module(\"linear2\",nn.Linear(128,2))\n",
    "    model1_BS1 = model1_BS1.cuda()\n",
    "\n",
    "    model1_BS2 = nn.Sequential()\n",
    "    model1_BS2.add_module(\"resnet_block1\", resnet_block(3, 32))\n",
    "    model1_BS2.add_module(\"resnet_block2\", resnet_block(32, 64))\n",
    "    model1_BS2.add_module(\"flatten\",nn.Flatten())\n",
    "    model1_BS2.add_module(\"Dropout\",nn.Dropout(0.5))\n",
    "    model1_BS2.add_module(\"linear0\",nn.Linear(64*16*13,32))\n",
    "    model1_BS2.add_module(\"linear1\",nn.Linear(32,128))\n",
    "    model1_BS2.add_module(\"ReLU1\",nn.ReLU())\n",
    "    model1_BS2.add_module(\"linear2\",nn.Linear(128,2))\n",
    "    model1_BS2 = model1_BS2.cuda()\n",
    "    \n",
    "else:\n",
    "    #Read in the pretrained model. You may need to modify the root path of the pretrained models\n",
    "    model1_BS1 = torch.load('model1_BS1_wellTrained_300data.pth')\n",
    "    model1_BS2 = torch.load('model1_BS2_wellTrained_300data.pth')\n",
    "    model1_BS1 = model1_BS1.cuda()\n",
    "    model1_BS2 = model1_BS2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0dd451d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#                Training Setups            #\n",
    "#############################################\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer_BS1 = torch.optim.Adam(model1_BS1.parameters(), lr = learning_rate, weight_decay=0.001)\n",
    "optimizer_BS2 = torch.optim.Adam(model1_BS2.parameters(), lr = learning_rate, weight_decay=0.001)\n",
    "\n",
    "lambda_BS1 = lambda epoch: 0.9 ** epoch\n",
    "lambda_BS2 = lambda epoch: 0.9 ** epoch\n",
    "\n",
    "scheduler_BS1 = torch.optim.lr_scheduler.LambdaLR(optimizer_BS1,lr_lambda=lambda_BS1)\n",
    "scheduler_BS2 = torch.optim.lr_scheduler.LambdaLR(optimizer_BS2,lr_lambda=lambda_BS2)\n",
    "\n",
    "criterion_BS1 = nn.MSELoss()\n",
    "criterion_BS2 = nn.MSELoss()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    criterion_BS1 = criterion_BS1.cuda()\n",
    "    criterion_BS2 = criterion_BS2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "74356f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#         Initialize the weights            #\n",
    "#############################################\n",
    "\n",
    "model1_BS1.eval()\n",
    "val_step = 1\n",
    "    \n",
    "val_output_all_bs1 = torch.tensor([])\n",
    "val_labels_all_bs1 = torch.tensor([])\n",
    "for val_step, (val_inputs,val_labels) in enumerate(valid_dl_BS1, 1):\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model1_BS1(val_inputs.cuda())\n",
    "        val_output_all_bs1 = torch.cat([val_output_all_bs1,val_outputs.cpu()],dim=0)\n",
    "        val_labels_all_bs1 = torch.cat([val_labels_all_bs1,val_labels],dim=0)\n",
    "            \n",
    "distance_bs1 = torch.norm(val_labels_all_bs1,dim=1)\n",
    "error_bs1 = torch.norm(val_output_all_bs1-val_labels_all_bs1,dim=1)\n",
    "\n",
    "an_bs1 = np.polyfit(distance_bs1.numpy(),error_bs1.numpy(), 3)\n",
    "\n",
    "\n",
    "\n",
    "model1_BS2.eval()\n",
    "val_step = 1\n",
    "    \n",
    "val_output_all_bs2 = torch.tensor([])\n",
    "val_labels_all_bs2 = torch.tensor([])\n",
    "for val_step, (val_inputs,val_labels) in enumerate(valid_dl_BS2, 1):\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model1_BS2(val_inputs.cuda())\n",
    "        val_output_all_bs2 = torch.cat([val_output_all_bs2,val_outputs.cpu()],dim=0)\n",
    "        val_labels_all_bs2 = torch.cat([val_labels_all_bs2,val_labels],dim=0)\n",
    "            \n",
    "distance_bs2 = torch.norm(val_labels_all_bs2,dim=1)\n",
    "error_bs2 = torch.norm(val_output_all_bs2-val_labels_all_bs2,dim=1)\n",
    "\n",
    "an_bs2 = np.polyfit(distance_bs2.numpy(),error_bs2.numpy(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2ad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#               Model Training              #\n",
    "#############################################\n",
    "\n",
    "batchSize = 24\n",
    "Steps = 10000\n",
    "args = {'bs':100, 'lr':1e-3, 'n_epochs':150, 'device':'cuda:0'}\n",
    "test_result_bs1 = []\n",
    "test_result_bs2 = []\n",
    "bs1_coor = torch.tensor([80,14])\n",
    "bs2_coor = torch.tensor([140,-14])\n",
    "\n",
    "#Whether use the meta learning\n",
    "using_meta_learning = True\n",
    "\n",
    "#ReLU or proposed\n",
    "whether_using_ReLU = False\n",
    "\n",
    "for step in range(1,Steps+1):\n",
    "    #Zero grad\n",
    "    optimizer_BS1.zero_grad()\n",
    "    optimizer_BS2.zero_grad()\n",
    "    \n",
    "    #training phase\n",
    "    model1_BS1.train()\n",
    "    model1_BS2.train()\n",
    "    #random select several bags\n",
    "    idx_step = torch.randperm(len(dataset_tensor))[:batchSize]\n",
    "    \n",
    "    #We need to initialize a tensor for integrating all the selected bags\n",
    "    input_holder_all_bs1 = torch.tensor([])\n",
    "    input_holder_all_bs2 = torch.tensor([])\n",
    "    \n",
    "    #number of elements in each bag\n",
    "    number_of_instance_bs1 = []\n",
    "    number_of_instance_bs2 = []\n",
    "    \n",
    "    #concat all the bags for batch normalization\n",
    "    for item in idx_step:\n",
    "        input_holder_all_bs1 = torch.cat([input_holder_all_bs1,dataset_tensor[item][0][0]],dim=0)\n",
    "        input_holder_all_bs2 = torch.cat([input_holder_all_bs2,dataset_tensor[item][1][0]],dim=0)\n",
    "        \n",
    "        number_of_instance_bs1.append(dataset_tensor[item][0][0].shape[0])\n",
    "        number_of_instance_bs2.append(dataset_tensor[item][1][0].shape[0])\n",
    "    \n",
    "    #go through the NN\n",
    "    output_holder_all_bs1 = model1_BS1(input_holder_all_bs1.cuda())\n",
    "    output_holder_all_bs2 = model1_BS2(input_holder_all_bs2.cuda())\n",
    "    \n",
    "    #re-divide the outputs into bags\n",
    "    output_divide_bs1 = dividing(output_holder_all_bs1.cpu().detach()+bs1_coor.repeat(output_holder_all_bs1.shape[0],1),number_of_instance_bs1)\n",
    "    output_divide_bs2 = dividing(output_holder_all_bs2.cpu().detach()+bs2_coor.repeat(output_holder_all_bs2.shape[0],1),number_of_instance_bs2)\n",
    "    \n",
    "    #Calculating the matching through KM algorithm\n",
    "    with torch.no_grad():\n",
    "        label_holder_bs1 = torch.tensor([])\n",
    "        label_holder_bs2 = torch.tensor([])\n",
    "        \n",
    "        for j in range(len(output_divide_bs1)):\n",
    "            position_pred_bs1 = output_divide_bs1[j]\n",
    "            position_pred_bs2 = output_divide_bs2[j]\n",
    "            position_pred_all = torch.cat([position_pred_bs1,position_pred_bs2],dim=0)\n",
    "        \n",
    "            position_image_all = torch.cat([dataset_tensor[idx_step[j]][0][1],dataset_tensor[idx_step[j]][1][1]],dim=0)\n",
    "        \n",
    "            distance_relativeto_bs1 = torch.norm(position_image_all - bs1_coor.repeat(position_image_all.shape[0],1),dim=1)\n",
    "            distance_relativeto_bs2 = torch.norm(position_image_all - bs2_coor.repeat(position_image_all.shape[0],1),dim=1)\n",
    "        \n",
    "            weighingVector_bs1 = torch.from_numpy(np.polyval(an_bs1,distance_relativeto_bs1.numpy()))\n",
    "            weighingVector_bs2 = torch.from_numpy(np.polyval(an_bs2,distance_relativeto_bs2.numpy()))\n",
    "        \n",
    "            stdDeviation_matrix = torch.cat([weighingVector_bs1.repeat(position_pred_bs1.shape[0],1),weighingVector_bs2.repeat(position_pred_bs2.shape[0],1)],dim=0)\n",
    "            row_ind, col_ind, _ = E_step(position_pred_all,position_image_all,stdDeviation_matrix)\n",
    "        \n",
    "            position_pred_all_copy = position_pred_all.clone()\n",
    "            position_pred_all_copy[row_ind,:]=position_image_all[col_ind,:]\n",
    "        \n",
    "            label_partial_bs1 = position_pred_all_copy[:position_pred_bs1.shape[0],:]\n",
    "            label_partial_bs2 = position_pred_all_copy[-1*position_pred_bs2.shape[0]:,:]\n",
    "            \n",
    "            label_holder_bs1 = torch.cat([label_holder_bs1,label_partial_bs1-bs1_coor.repeat(label_partial_bs1.shape[0],1)],dim=0)\n",
    "            label_holder_bs2 = torch.cat([label_holder_bs2,label_partial_bs2-bs2_coor.repeat(label_partial_bs2.shape[0],1)],dim=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Check it\n",
    "    if output_holder_all_bs1.shape[0]!=label_holder_bs1.shape[0] or output_holder_all_bs2.shape[0]!=label_holder_bs2.shape[0]:\n",
    "        print('Wrong!')\n",
    "        break\n",
    "    \n",
    "    #Select some clean labeled data for regularization\n",
    "    idx_BS1 = torch.randperm(train_dl_BS1.dataset[:][0].shape[0])[:min(label_holder_bs1.shape[0],train_dl_BS1.dataset[:][0].shape[0])]\n",
    "    input_reg_bs1 = train_dl_BS1.dataset[:][0][idx_BS1,:,:,:]\n",
    "    labels_reg_bs1 = train_dl_BS1.dataset[:][1][idx_BS1,:]\n",
    "    idx_BS2 = torch.randperm(train_dl_BS2.dataset[:][0].shape[0])[:min(label_holder_bs2.shape[0],train_dl_BS2.dataset[:][0].shape[0])]\n",
    "    input_reg_bs2 = train_dl_BS2.dataset[:][0][idx_BS2,:,:,:]\n",
    "    labels_reg_bs2 = train_dl_BS2.dataset[:][1][idx_BS2,:]\n",
    "    output_reg_bs1 = model1_BS1(input_reg_bs1.cuda())\n",
    "    output_reg_bs2 = model1_BS2(input_reg_bs2.cuda())\n",
    "          \n",
    "    #Meta-learning based re-weighting\n",
    "    if using_meta_learning is False:\n",
    "        loss_bs1 = 0.5*criterion_BS1(output_holder_all_bs1,label_holder_bs1.cuda()) + 0.5*criterion_BS1(output_reg_bs1,labels_reg_bs1.cuda())\n",
    "        loss_bs2 = 0.5*criterion_BS2(output_holder_all_bs2,label_holder_bs2.cuda()) + 0.5*criterion_BS2(output_reg_bs2,labels_reg_bs2.cuda())\n",
    "        \n",
    "    else:\n",
    "        #Select some clean validation data\n",
    "        idx_BS1_meta = torch.randperm(valid_dl_BS1.dataset[:][0].shape[0])[:min(label_holder_bs1.shape[0],valid_dl_BS1.dataset[:][0].shape[0])]\n",
    "        input_meta_bs1 = valid_dl_BS1.dataset[:][0][idx_BS1_meta,:,:,:]\n",
    "        labels_meta_bs1 = valid_dl_BS1.dataset[:][1][idx_BS1_meta,:]\n",
    "        idx_BS2_meta = torch.randperm(valid_dl_BS2.dataset[:][0].shape[0])[:min(label_holder_bs2.shape[0],valid_dl_BS2.dataset[:][0].shape[0])]\n",
    "        input_meta_bs2 = valid_dl_BS2.dataset[:][0][idx_BS2_meta,:,:,:]\n",
    "        labels_meta_bs2 = valid_dl_BS2.dataset[:][1][idx_BS2_meta,:]\n",
    "        \n",
    "        #Meta-learning For BS1\n",
    "        with higher.innerloop_ctx(model1_BS1, optimizer_BS1) as (meta_model_bs1, meta_opt_bs1):\n",
    "            meta_train_outputs_bs1 = meta_model_bs1(input_holder_all_bs1.cuda())\n",
    "            meta_train_loss_bs1 = torch.square(torch.norm(meta_train_outputs_bs1-label_holder_bs1.cuda(),dim=1))\n",
    "            eps_bs1 = torch.zeros(meta_train_loss_bs1.size(), requires_grad=True, device=args['device'])\n",
    "            meta_train_loss_bs1 = torch.sum(eps_bs1 * meta_train_loss_bs1)\n",
    "            meta_opt_bs1.step(meta_train_loss_bs1)\n",
    "\n",
    "            meta_val_outputs_bs1 = meta_model_bs1(input_meta_bs1.cuda())\n",
    "            meta_val_loss_bs1 = torch.square(torch.norm(meta_val_outputs_bs1-labels_meta_bs1.cuda(),dim=1)).mean()\n",
    "            eps_grads_bs1 = torch.autograd.grad(meta_val_loss_bs1, eps_bs1)[0].detach()\n",
    "        \n",
    "\n",
    "        if whether_using_ReLU is True:\n",
    "            w_tilde_bs1 = torch.clamp(-eps_grads_bs1, min=0)\n",
    "            num_bs1_nonzero = torch.count_nonzero(w_tilde_bs1)\n",
    "            l1_norm_bs1 = torch.sum(w_tilde_bs1)\n",
    "            if l1_norm_bs1 != 0:\n",
    "                w_bs1 = (w_tilde_bs1.shape[0]/num_bs1_nonzero) * w_tilde_bs1 / l1_norm_bs1\n",
    "            else:\n",
    "                w_bs1 = w_tilde_bs1\n",
    "                \n",
    "        elif whether_using_ReLU is False:\n",
    "            w_tilde_bs1 = -eps_grads_bs1/(torch.max(torch.abs(eps_grads_bs1)))\n",
    "            w_bs1 = torch.ones(w_tilde_bs1.size()).cuda() + w_tilde_bs1\n",
    "        \n",
    "\n",
    "        #Meta learning For BS2\n",
    "        with higher.innerloop_ctx(model1_BS2, optimizer_BS2) as (meta_model_bs2, meta_opt_bs2):\n",
    "            meta_train_outputs_bs2 = meta_model_bs2(input_holder_all_bs2.cuda())\n",
    "            meta_train_loss_bs2 = torch.square(torch.norm(meta_train_outputs_bs2-label_holder_bs2.cuda(),dim=1))\n",
    "            eps_bs2 = torch.zeros(meta_train_loss_bs2.size(), requires_grad=True, device=args['device'])\n",
    "            meta_train_loss_bs2 = torch.sum(eps_bs2 * meta_train_loss_bs2)\n",
    "            meta_opt_bs2.step(meta_train_loss_bs2)\n",
    "\n",
    "            meta_val_outputs_bs2 = meta_model_bs2(input_meta_bs2.cuda())\n",
    "            meta_val_loss_bs2 = torch.square(torch.norm(meta_val_outputs_bs2-labels_meta_bs2.cuda())).mean()\n",
    "            eps_grads_bs2 = torch.autograd.grad(meta_val_loss_bs2, eps_bs2)[0].detach()\n",
    "        \n",
    "        if whether_using_ReLU is True:\n",
    "            w_tilde_bs2 = torch.clamp(-eps_grads_bs2, min=0)\n",
    "            num_bs2_nonzero = torch.count_nonzero(w_tilde_bs2)\n",
    "            l1_norm_bs2 = torch.sum(w_tilde_bs2)\n",
    "            if l1_norm_bs2 != 0:\n",
    "                w_bs2 = (w_tilde_bs2.shape[0]/num_bs2_nonzero) * w_tilde_bs2 / l1_norm_bs2\n",
    "            else:\n",
    "                w_bs2 = w_tilde_bs2\n",
    "            \n",
    "        elif whether_using_ReLU is False:\n",
    "            w_tilde_bs2 = -eps_grads_bs2/(torch.max(torch.abs(eps_grads_bs2)))\n",
    "            w_bs2 = torch.ones(w_tilde_bs2.size()).cuda() + w_tilde_bs2\n",
    "        \n",
    "\n",
    "        \n",
    "        loss_bs1_weakly = torch.square(torch.norm(output_holder_all_bs1-label_holder_bs1.cuda(),dim=1))/2\n",
    "        loss_bs1_weakly = torch.mean(w_bs1 * loss_bs1_weakly)\n",
    "        \n",
    "        loss_bs2_weakly = torch.square(torch.norm(output_holder_all_bs2-label_holder_bs2.cuda(),dim=1))/2\n",
    "        loss_bs2_weakly = torch.mean(w_bs2 * loss_bs2_weakly)\n",
    "        \n",
    "        loss_bs1 = 0.5*loss_bs1_weakly + 0.5 * criterion_BS1(output_reg_bs1,labels_reg_bs1.cuda())\n",
    "        loss_bs2 = 0.5*loss_bs2_weakly + 0.5 * criterion_BS2(output_reg_bs2,labels_reg_bs2.cuda())\n",
    "    \n",
    "    #Optimizing the NNs\n",
    "    loss_bs1.backward()\n",
    "    loss_bs2.backward()\n",
    "    \n",
    "    optimizer_BS1.step()\n",
    "    optimizer_BS2.step()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    #Lr decay\n",
    "    if step%200==0 and step>=5000:\n",
    "        scheduler_BS1.step()\n",
    "        scheduler_BS2.step()\n",
    "       \n",
    "    \n",
    "    #evaluation the performance on testing datasets.\n",
    "    #Note that generally, a testing dataset is not used until the model is well-trained. \n",
    "    #But the validation in our work is used to calculate the weights for each positions,\n",
    "    #so we evaluate the performance with the testing datasets\n",
    "    if step%100==0:\n",
    "        #验证阶段\n",
    "        model1_BS1.eval()\n",
    "        model1_BS2.eval()\n",
    "\n",
    "        test_metric_sum_bs1 = 0.0\n",
    "        test_step_bs1 = 1\n",
    "    \n",
    "        test_output_all_bs1 = torch.tensor([])\n",
    "        test_labels_all_bs1 = torch.tensor([])\n",
    "        for test_step_bs1, (test_inputs_bs1,test_labels_bs1) in enumerate(testing_dl_BS1, 1):\n",
    "            with torch.no_grad():\n",
    "                test_outputs_bs1 = model1_BS1(test_inputs_bs1.cuda())\n",
    "                test_metric_bs1 = metric_func(test_outputs_bs1,test_labels_bs1.cuda())\n",
    "                test_output_all_bs1 = torch.cat([test_output_all_bs1,test_outputs_bs1.cpu()],dim=0)\n",
    "                test_labels_all_bs1 = torch.cat([test_labels_all_bs1,test_labels_bs1],dim=0)\n",
    "\n",
    "            test_metric_sum_bs1 += test_metric_bs1\n",
    "\n",
    "\n",
    "        test_metric_sum_bs2 = 0.0\n",
    "        test_step_bs2 = 1\n",
    "        test_output_all_bs2 = torch.tensor([])\n",
    "        test_labels_all_bs2 = torch.tensor([])\n",
    "        for test_step_bs2, (test_inputs_bs2,test_labels_bs2) in enumerate(testing_dl_BS2, 1):\n",
    "            with torch.no_grad():\n",
    "                test_outputs_bs2 = model1_BS2(test_inputs_bs2.cuda())\n",
    "                test_metric_bs2 = metric_func(test_outputs_bs2,test_labels_bs2.cuda())\n",
    "                test_output_all_bs2 = torch.cat([test_output_all_bs2,test_outputs_bs2.cpu()],dim=0)\n",
    "                test_labels_all_bs2 = torch.cat([test_labels_all_bs2,test_labels_bs2],dim=0)\n",
    "            test_metric_sum_bs2 += test_metric_bs2\n",
    "            \n",
    "        info = (step,test_metric_sum_bs1/test_step_bs1,test_metric_sum_bs2/test_step_bs2)\n",
    "\n",
    "        print((\"\\nSTEP = %d,test_metric_bs1 = %.3f,test_metric_bs2 = %.3f\\n\") \n",
    "              %info)\n",
    "        \n",
    "        test_result_bs1.append((test_metric_sum_bs1/test_step_bs1).item())\n",
    "        test_result_bs2.append((test_metric_sum_bs2/test_step_bs2).item())\n",
    "    else:\n",
    "        print('step {},loss_bs1 {},loss_bs2 {}'.format(step, loss_bs1.item(), loss_bs2.item()))\n",
    "        \n",
    "    \n",
    "    if step%50==0:\n",
    "        #Re-calculate the sigma function with polyfit\n",
    "        model1_BS1.eval()\n",
    "        model1_BS2.eval()\n",
    "    \n",
    "        val_output_all_bs1 = torch.tensor([])\n",
    "        val_labels_all_bs1 = torch.tensor([])\n",
    "        for val_step_bs1, (val_inputs_bs1,val_labels_bs1) in enumerate(valid_dl_BS1, 1):\n",
    "            with torch.no_grad():\n",
    "                val_outputs_bs1 = model1_BS1(val_inputs_bs1.cuda())\n",
    "                val_output_all_bs1 = torch.cat([val_output_all_bs1,val_outputs_bs1.cpu()],dim=0)\n",
    "                val_labels_all_bs1 = torch.cat([val_labels_all_bs1,val_labels_bs1],dim=0)\n",
    "            \n",
    "        distance_bs1 = torch.norm(val_labels_all_bs1,dim=1)\n",
    "        error_bs1 = torch.norm(val_output_all_bs1-val_labels_all_bs1,dim=1)\n",
    "        an_bs1 = np.polyfit(distance_bs1.numpy(),error_bs1.numpy(), 3)\n",
    "        \n",
    "        val_output_all_bs2 = torch.tensor([])\n",
    "        val_labels_all_bs2 = torch.tensor([])\n",
    "        for val_step_bs2, (val_inputs_bs2,val_labels_bs2) in enumerate(valid_dl_BS2, 1):\n",
    "            with torch.no_grad():\n",
    "                val_outputs_bs2 = model1_BS2(val_inputs_bs2.cuda())\n",
    "                val_output_all_bs2 = torch.cat([val_output_all_bs2,val_outputs_bs2.cpu()],dim=0)\n",
    "                val_labels_all_bs2 = torch.cat([val_labels_all_bs2,val_labels_bs2],dim=0)\n",
    "            \n",
    "        distance_bs2 = torch.norm(val_labels_all_bs2,dim=1)\n",
    "        error_bs2 = torch.norm(val_output_all_bs2-val_labels_all_bs2,dim=1)\n",
    "        an_bs2 = np.polyfit(distance_bs2.numpy(),error_bs2.numpy(), 3)\n",
    "        \n",
    "    torch.cuda.empty_cache()  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d8ba92e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#               Model Testing               #\n",
    "#############################################\n",
    "\n",
    "#Read in multi-modal testing dataset\n",
    "dataset_tensor_testing = np.load('multi_modal_testSet.npy',allow_pickle=True)\n",
    "\n",
    "#Coordinates of BS\n",
    "bs1_coor = torch.tensor([80,14])\n",
    "bs2_coor = torch.tensor([140,-14])\n",
    "\n",
    "#Initialize sum of metrics\n",
    "test_metric_sum_bs1_withImage_collective = 0.0\n",
    "test_metric_sum_bs1_noImage = 0.0\n",
    "test_metric_sum_bs1_withImage_collective_nearest = 0.0\n",
    "\n",
    "test_metric_sum_bs2_withImage_collective = 0.0\n",
    "test_metric_sum_bs2_noImage = 0.0\n",
    "test_metric_sum_bs2_withImage_collective_nearest = 0.0\n",
    "test_step = 1\n",
    "\n",
    "#Switch the mode\n",
    "model1_BS1.eval()\n",
    "model1_BS2.eval()\n",
    "\n",
    "#intialize tensor for recording the testing results\n",
    "test_labels_bs1_holder = torch.tensor([])\n",
    "test_labels_bs2_holder = torch.tensor([])\n",
    "test_output_bs1_holder = torch.tensor([])\n",
    "test_output_bs2_holder = torch.tensor([])\n",
    "test_calibrated_bs1_holder = torch.tensor([])\n",
    "test_calibrated_bs2_holder = torch.tensor([])\n",
    "test_calibrated_bs1_holder_collective_nearest = torch.tensor([])\n",
    "test_calibrated_bs2_holder_collective_nearest = torch.tensor([])\n",
    "\n",
    "sum_num_of_chosen_bs1 = 0\n",
    "sum_num_of_chosen_bs2 = 0\n",
    "\n",
    "for test_step, ((test_inputs_bs1,test_labels_bs1,test_labels_bs1_chosen),(test_inputs_bs2,test_labels_bs2,test_labels_bs2_chosen)) in enumerate(dataset_tensor_testing, 1):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_outputs_bs1 = model1_BS1(test_inputs_bs1.cuda())\n",
    "        test_labels_bs1_relative = test_labels_bs1 - bs1_coor.repeat(test_labels_bs1.shape[0],1)\n",
    "        test_metric_bs1_noImage = metric_func(test_outputs_bs1,test_labels_bs1_relative.cuda())\n",
    "        test_metric_sum_bs1_noImage += test_metric_bs1_noImage\n",
    "        test_labels_bs1_holder = torch.cat([test_labels_bs1_holder,test_labels_bs1_relative],dim=0)\n",
    "        test_output_bs1_holder = torch.cat([test_output_bs1_holder,test_outputs_bs1.detach().cpu()],dim=0)\n",
    "            \n",
    "        test_outputs_bs2 = model1_BS2(test_inputs_bs2.cuda())\n",
    "        test_labels_bs2_relative = test_labels_bs2 - bs2_coor.repeat(test_labels_bs2.shape[0],1)\n",
    "        test_metric_bs2_noImage = metric_func(test_outputs_bs2,test_labels_bs2_relative.cuda())\n",
    "        test_metric_sum_bs2_noImage += test_metric_bs2_noImage\n",
    "        test_labels_bs2_holder = torch.cat([test_labels_bs2_holder,test_labels_bs2_relative],dim=0)\n",
    "        test_output_bs2_holder = torch.cat([test_output_bs2_holder,test_outputs_bs2.detach().cpu()],dim=0)\n",
    "            \n",
    "    \n",
    "            \n",
    "        sum_num_of_chosen_bs1 = sum_num_of_chosen_bs1 + len(test_labels_bs1_chosen)\n",
    "        sum_num_of_chosen_bs2 = sum_num_of_chosen_bs2 + len(test_labels_bs2_chosen)\n",
    "\n",
    "            \n",
    "        test_labels_chosen = torch.cat([test_labels_bs1_chosen,test_labels_bs2_chosen],dim=0)\n",
    "        distance_relativeto_bs1 = torch.norm(test_labels_chosen - bs1_coor.repeat(test_labels_chosen.shape[0],1),dim=1)\n",
    "        distance_relativeto_bs2 = torch.norm(test_labels_chosen - bs2_coor.repeat(test_labels_chosen.shape[0],1),dim=1)\n",
    "            \n",
    "        weighingVector_bs1 = torch.from_numpy(np.polyval(an_bs1,distance_relativeto_bs1.numpy()))\n",
    "        weighingVector_bs2 = torch.from_numpy(np.polyval(an_bs2,distance_relativeto_bs2.numpy()))\n",
    "        \n",
    "        weighingMatrix_bs1 = weighingVector_bs1.repeat(test_inputs_bs1.shape[0],1)\n",
    "        weighingMatrix_bs2 = weighingVector_bs2.repeat(test_inputs_bs2.shape[0],1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #求解一起匹配的解\n",
    "        test_outputs_all = torch.cat([test_outputs_bs1.cpu()+bs1_coor.repeat(test_outputs_bs1.shape[0],1),test_outputs_bs2.cpu()+bs2_coor.repeat(test_outputs_bs2.shape[0],1)],dim=0)\n",
    "        weighingMatrix_all = torch.cat([weighingMatrix_bs1,weighingMatrix_bs2],dim=0)\n",
    "        row_ind_all, col_ind_all,root_matrix = E_step(test_outputs_all,test_labels_chosen,weighingMatrix_all)\n",
    "        \n",
    "        test_outputs_all_calibrate = test_outputs_all.clone()\n",
    "        test_outputs_all_calibrate[row_ind_all,:] = test_labels_chosen[col_ind_all,:]\n",
    "        \n",
    "        test_outputs_bs1_calibrate_collective = test_outputs_all_calibrate[:test_outputs_bs1.shape[0],:] - bs1_coor.repeat(test_outputs_bs1.shape[0],1)\n",
    "        test_outputs_bs2_calibrate_collective = test_outputs_all_calibrate[-1*test_outputs_bs2.shape[0]:,:] - bs2_coor.repeat(test_outputs_bs2.shape[0],1)\n",
    "        \n",
    "        test_metric_bs1_withImage_collective = metric_func(test_labels_bs1_relative,test_outputs_bs1_calibrate_collective)\n",
    "        test_metric_sum_bs1_withImage_collective += test_metric_bs1_withImage_collective\n",
    "        test_calibrated_bs1_holder = torch.cat([test_calibrated_bs1_holder,test_outputs_bs1_calibrate_collective],dim=0)\n",
    "        \n",
    "        test_metric_bs2_withImage_collective = metric_func(test_labels_bs2_relative,test_outputs_bs2_calibrate_collective)\n",
    "        test_metric_sum_bs2_withImage_collective += test_metric_bs2_withImage_collective\n",
    "        test_calibrated_bs2_holder = torch.cat([test_calibrated_bs2_holder,test_outputs_bs2_calibrate_collective],dim=0)\n",
    "        \n",
    "        row_ind_all_nearest, col_ind_all_nearest = Nearest(test_outputs_all,test_labels_chosen,weighingMatrix_all)\n",
    "        test_outputs_all_calibrate_nearest = test_outputs_all.clone()\n",
    "        test_outputs_all_calibrate_nearest[row_ind_all_nearest,:] = test_labels_chosen[col_ind_all_nearest,:]\n",
    "        test_outputs_bs1_calibrate_collective_nearest = test_outputs_all_calibrate_nearest[:test_outputs_bs1.shape[0],:] - bs1_coor.repeat(test_outputs_bs1.shape[0],1)\n",
    "        test_outputs_bs2_calibrate_collective_nearest = test_outputs_all_calibrate_nearest[-1*test_outputs_bs2.shape[0]:,:] - bs2_coor.repeat(test_outputs_bs2.shape[0],1)\n",
    "        test_metric_bs1_withImage_collective_nearest = metric_func(test_labels_bs1_relative,test_outputs_bs1_calibrate_collective_nearest)\n",
    "        test_metric_sum_bs1_withImage_collective_nearest += test_metric_bs1_withImage_collective_nearest\n",
    "        test_metric_bs2_withImage_collective_nearest = metric_func(test_labels_bs2_relative,test_outputs_bs2_calibrate_collective_nearest)\n",
    "        test_metric_sum_bs2_withImage_collective_nearest += test_metric_bs2_withImage_collective_nearest\n",
    "        \n",
    "        test_calibrated_bs1_holder_collective_nearest = torch.cat([test_calibrated_bs1_holder_collective_nearest,test_outputs_bs1_calibrate_collective_nearest],dim=0)\n",
    "        test_calibrated_bs2_holder_collective_nearest = torch.cat([test_calibrated_bs2_holder_collective_nearest,test_outputs_bs2_calibrate_collective_nearest],dim=0)\n",
    "        \n",
    "    \n",
    "#Calculating all the metrics\n",
    "distance_limit = 5\n",
    "test_error_Directly_bs1 = torch.norm(test_output_bs1_holder - test_labels_bs1_holder,dim=1)\n",
    "test_error_KM_bs1 = torch.norm(test_calibrated_bs1_holder - test_labels_bs1_holder,dim=1)\n",
    "test_error_Nearest_bs1 = torch.norm(test_calibrated_bs1_holder_collective_nearest - test_labels_bs1_holder,dim=1)\n",
    "test_error_Nearest_bs1_calibrate = test_error_Nearest_bs1.clone()\n",
    "for i in range(len(test_error_Nearest_bs1_calibrate)):\n",
    "    if test_error_Nearest_bs1_calibrate[i]>=distance_limit:\n",
    "        test_error_Nearest_bs1_calibrate[i]=test_error_Directly_bs1[i]\n",
    "\n",
    "test_error_Directly_bs2 = torch.norm(test_output_bs2_holder - test_labels_bs2_holder,dim=1)\n",
    "test_error_KM_bs2 = torch.norm(test_calibrated_bs2_holder - test_labels_bs2_holder,dim=1)\n",
    "test_error_Nearest_bs2 = torch.norm(test_calibrated_bs2_holder_collective_nearest - test_labels_bs2_holder,dim=1)\n",
    "test_error_Nearest_bs2_calibrate = test_error_Nearest_bs2.clone()\n",
    "for i in range(len(test_error_Nearest_bs2_calibrate)):\n",
    "    if test_error_Nearest_bs2_calibrate[i]>=distance_limit:\n",
    "        test_error_Nearest_bs2_calibrate[i]=test_error_Directly_bs2[i]\n",
    "    \n",
    "#Metrics of BS1\n",
    "test_metric_mean_bs1_withImage_collective = torch.mean(test_error_KM_bs1)\n",
    "test_metric_mean_bs1_noImage = torch.mean(test_error_Directly_bs1)\n",
    "test_metric_mean_bs1_withImage_collective_nearest = torch.mean(test_error_Nearest_bs1)\n",
    "test_metric_mean_bs1_withImage_collective_nearest_calibrate = torch.mean(test_error_Nearest_bs1_calibrate)\n",
    "\n",
    "#Metrics of BS2\n",
    "test_metric_mean_bs2_withImage_collective = torch.mean(test_error_KM_bs2)\n",
    "test_metric_mean_bs2_noImage = torch.mean(test_error_Directly_bs2)\n",
    "test_metric_mean_bs2_withImage_collective_nearest = torch.mean(test_error_Nearest_bs2)\n",
    "test_metric_mean_bs2_withImage_collective_nearest_calibrate = torch.mean(test_error_Nearest_bs2_calibrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e88d71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "45982009",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
